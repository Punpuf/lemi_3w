{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento com dados de todos os poços\n",
    "\n",
    "Nesse notebook será realizado as tarefas de obtenção de dados, tratamento, modelagem, e validação de dados.\n",
    "O objetivo aqui é obter um classificador de anomalias para dados de todos os poços (aqui será incluido todos os tipos de dados: inclusive dados de fontes simulação e desenhados)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquisição de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurando ambiente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 18:07:37.102043: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-11 18:07:37.105291: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 18:07:37.183559: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 18:07:37.185507: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 18:07:38.660810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Environment configuration\n",
    "import raw_data_manager.raw_data_acquisition as rda\n",
    "import raw_data_manager.raw_data_inspector as rdi\n",
    "import raw_data_manager.raw_data_splitter as rds\n",
    "from data_exploration.metric_acquisition import MetricAcquisition\n",
    "from data_preparation.transformation_manager import TransformationManager\n",
    "from constants import utils, config\n",
    "import pathlib\n",
    "\n",
    "# Set default logging level.\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.DEBUG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar dataset 3W (se não disponível) & gerar tabela de metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Directory with the biggest version: /home/ubuntu/lemi_3w/data/dataset_converted_v10101\n",
      "INFO:absl:Version: 10101\n",
      "INFO:absl:Latest local version is 10101\n",
      "INFO:absl:Going to fetch config file from $https://raw.githubusercontent.com/petrobras/3W/main/dataset/dataset.ini\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2786  100  2786    0     0   3750      0 --:--:-- --:--:-- --:--:--  3749\n",
      "INFO:absl:Latest online version is 10101\n",
      "INFO:absl:Found existing converted data with dataset version of 10101\n",
      "INFO:absl:Directory with the biggest version: /home/ubuntu/lemi_3w/data/dataset_converted_v10101\n",
      "INFO:absl:Version: 10101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>source</th>\n",
       "      <th>well_id</th>\n",
       "      <th>path</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>file_size</th>\n",
       "      <th>num_timesteps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hash_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74203bb</th>\n",
       "      <td>NORMAL</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>2017-05-24 03:00:00</td>\n",
       "      <td>491415</td>\n",
       "      <td>17885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9fbd6f9</th>\n",
       "      <td>NORMAL</td>\n",
       "      <td>REAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>2017-08-09 06:00:00</td>\n",
       "      <td>520154</td>\n",
       "      <td>17933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28804c5</th>\n",
       "      <td>NORMAL</td>\n",
       "      <td>REAL</td>\n",
       "      <td>6.0</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>2017-05-08 09:00:31</td>\n",
       "      <td>349162</td>\n",
       "      <td>17970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42afe91</th>\n",
       "      <td>NORMAL</td>\n",
       "      <td>REAL</td>\n",
       "      <td>8.0</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>2017-07-01 14:01:35</td>\n",
       "      <td>251880</td>\n",
       "      <td>17799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa71d94</th>\n",
       "      <td>NORMAL</td>\n",
       "      <td>REAL</td>\n",
       "      <td>6.0</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>2017-08-23 19:00:00</td>\n",
       "      <td>279737</td>\n",
       "      <td>17949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ea66cf6</th>\n",
       "      <td>SEVERE_SLUGGING</td>\n",
       "      <td>SIMULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2315903</td>\n",
       "      <td>61999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34f032a</th>\n",
       "      <td>SEVERE_SLUGGING</td>\n",
       "      <td>SIMULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2259539</td>\n",
       "      <td>61999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876a969</th>\n",
       "      <td>SEVERE_SLUGGING</td>\n",
       "      <td>REAL</td>\n",
       "      <td>14.0</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>2017-09-25 06:00:42</td>\n",
       "      <td>1005717</td>\n",
       "      <td>17959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deac7ec</th>\n",
       "      <td>SEVERE_SLUGGING</td>\n",
       "      <td>SIMULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2045137</td>\n",
       "      <td>61999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c8867fa</th>\n",
       "      <td>SEVERE_SLUGGING</td>\n",
       "      <td>SIMULATED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/ubuntu/lemi_3w/data/dataset_converted_v1...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2359418</td>\n",
       "      <td>61999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1978 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              class_type     source  well_id  \\\n",
       "hash_id                                        \n",
       "74203bb           NORMAL       REAL      1.0   \n",
       "9fbd6f9           NORMAL       REAL      2.0   \n",
       "28804c5           NORMAL       REAL      6.0   \n",
       "42afe91           NORMAL       REAL      8.0   \n",
       "fa71d94           NORMAL       REAL      6.0   \n",
       "...                  ...        ...      ...   \n",
       "ea66cf6  SEVERE_SLUGGING  SIMULATED      NaN   \n",
       "34f032a  SEVERE_SLUGGING  SIMULATED      NaN   \n",
       "876a969  SEVERE_SLUGGING       REAL     14.0   \n",
       "deac7ec  SEVERE_SLUGGING  SIMULATED      NaN   \n",
       "c8867fa  SEVERE_SLUGGING  SIMULATED      NaN   \n",
       "\n",
       "                                                      path  \\\n",
       "hash_id                                                      \n",
       "74203bb  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "9fbd6f9  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "28804c5  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "42afe91  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "fa71d94  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "...                                                    ...   \n",
       "ea66cf6  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "34f032a  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "876a969  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "deac7ec  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "c8867fa  /home/ubuntu/lemi_3w/data/dataset_converted_v1...   \n",
       "\n",
       "                  timestamp  file_size  num_timesteps  \n",
       "hash_id                                                \n",
       "74203bb 2017-05-24 03:00:00     491415          17885  \n",
       "9fbd6f9 2017-08-09 06:00:00     520154          17933  \n",
       "28804c5 2017-05-08 09:00:31     349162          17970  \n",
       "42afe91 2017-07-01 14:01:35     251880          17799  \n",
       "fa71d94 2017-08-23 19:00:00     279737          17949  \n",
       "...                     ...        ...            ...  \n",
       "ea66cf6                 NaT    2315903          61999  \n",
       "34f032a                 NaT    2259539          61999  \n",
       "876a969 2017-09-25 06:00:42    1005717          17959  \n",
       "deac7ec                 NaT    2045137          61999  \n",
       "c8867fa                 NaT    2359418          61999  \n",
       "\n",
       "[1978 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Acquire data (of entire 3W dataset)\n",
    "rda.acquire_dataset_if_needed()\n",
    "latest_converted_data_path, latest_converted_data_version = (\n",
    "    rda.get_latest_local_converted_data_version(config.DIR_PROJECT_DATA)\n",
    ")\n",
    "\n",
    "# Helper to overview metadata (of entire 3W dataset)\n",
    "inspector_all_data = rdi.RawDataInspector(\n",
    "    latest_converted_data_path,\n",
    "    config.PATH_DATA_INSPECTOR_CACHE,\n",
    "    True\n",
    ")\n",
    "metadata_all_data = inspector_all_data.get_metadata_table()\n",
    "metadata_all_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir dados (de forma estratificada) em treinamento e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:size of train data: 1582 --- size of test data: 396\n",
      "DEBUG:absl:train path /home/ubuntu/lemi_3w/data/dataset_converted_v10101_split-20_source-all_class-all_well-all_train --- test path /home/ubuntu/lemi_3w/data/dataset_converted_v10101_split-20_source-all_class-all_well-all_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe3237ddcb64c00aa8bdf2e3fd33c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DONE:   0%|          | 0/1582 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf018980a7a405cbd8d20fc90494b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DONE:   0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# splits data, from the selected well, into train and test datasets\n",
    "splitter = rds.RawDataSplitter(metadata_all_data, latest_converted_data_version)\n",
    "split_train_dir, split_test_dir = splitter.stratefy_split_of_data(\n",
    "    data_dir=config.DIR_PROJECT_DATA, \n",
    "    test_size=0.20,\n",
    ")\n",
    "\n",
    "# generates metadata tables for split data\n",
    "train_metadata = rdi.RawDataInspector(\n",
    "    dataset_dir=split_train_dir,\n",
    "    cache_file_path=config.DIR_PROJECT_CACHE / \"train_metadata_all_data.parquet\",\n",
    "    use_cached=True\n",
    ")\n",
    "test_metadata = rdi.RawDataInspector(\n",
    "    dataset_dir=split_test_dir,\n",
    "    cache_file_path=config.DIR_PROJECT_CACHE / \"test_metadata_all_data.parquet\",\n",
    "    use_cached=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela de anomalias por tipo de fonte - treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_count</th>\n",
       "      <th>simul_count</th>\n",
       "      <th>drawn_count</th>\n",
       "      <th>soma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anomaly</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NORMAL</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABRUPT_INCREASE_BSW</th>\n",
       "      <td>4</td>\n",
       "      <td>91</td>\n",
       "      <td>8</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPURIOUS_CLOSURE_DHSV</th>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEVERE_SLUGGING</th>\n",
       "      <td>25</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOW_INSTABILITY</th>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAPID_PRODUCTIVITY_LOSS</th>\n",
       "      <td>9</td>\n",
       "      <td>351</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QUICK_RESTRICTION_PCK</th>\n",
       "      <td>5</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCALING_IN_PCK</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HYDRATE_IN_PRODUCTION_LINE</th>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>815</td>\n",
       "      <td>751</td>\n",
       "      <td>16</td>\n",
       "      <td>1582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            real_count  simul_count  drawn_count  soma\n",
       "anomaly                                                               \n",
       "NORMAL                             475            0            0   475\n",
       "ABRUPT_INCREASE_BSW                  4           91            8   103\n",
       "SPURIOUS_CLOSURE_DHSV               18           13            0    31\n",
       "SEVERE_SLUGGING                     25           59            0    84\n",
       "FLOW_INSTABILITY                   275            0            0   275\n",
       "RAPID_PRODUCTIVITY_LOSS              9          351            0   360\n",
       "QUICK_RESTRICTION_PCK                5          172            0   177\n",
       "SCALING_IN_PCK                       4            0            8    12\n",
       "HYDRATE_IN_PRODUCTION_LINE           0           65            0    65\n",
       "Total                              815          751           16  1582"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdi.RawDataInspector.generate_table_by_anomaly_source(train_metadata.get_metadata_table())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela de anomalias por tipo de fonte - teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_count</th>\n",
       "      <th>simul_count</th>\n",
       "      <th>drawn_count</th>\n",
       "      <th>soma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anomaly</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NORMAL</th>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABRUPT_INCREASE_BSW</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPURIOUS_CLOSURE_DHSV</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEVERE_SLUGGING</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOW_INSTABILITY</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAPID_PRODUCTIVITY_LOSS</th>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QUICK_RESTRICTION_PCK</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCALING_IN_PCK</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HYDRATE_IN_PRODUCTION_LINE</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>204</td>\n",
       "      <td>188</td>\n",
       "      <td>4</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            real_count  simul_count  drawn_count  soma\n",
       "anomaly                                                               \n",
       "NORMAL                             119            0            0   119\n",
       "ABRUPT_INCREASE_BSW                  1           23            2    26\n",
       "SPURIOUS_CLOSURE_DHSV                4            3            0     7\n",
       "SEVERE_SLUGGING                      7           15            0    22\n",
       "FLOW_INSTABILITY                    69            0            0    69\n",
       "RAPID_PRODUCTIVITY_LOSS              2           88            0    90\n",
       "QUICK_RESTRICTION_PCK                1           43            0    44\n",
       "SCALING_IN_PCK                       1            0            2     3\n",
       "HYDRATE_IN_PRODUCTION_LINE           0           16            0    16\n",
       "Total                              204          188            4   396"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdi.RawDataInspector.generate_table_by_anomaly_source(test_metadata.get_metadata_table())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamento de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegar valores da média dos valores e do desvio padrão. Foram calculados préviamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_of_means</th>\n",
       "      <th>mean_of_stds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P-PDG</th>\n",
       "      <td>1.650749e+07</td>\n",
       "      <td>1.201552e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-TPT</th>\n",
       "      <td>1.517584e+07</td>\n",
       "      <td>3.687992e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T-TPT</th>\n",
       "      <td>1.061237e+02</td>\n",
       "      <td>1.629953e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <td>4.729793e+06</td>\n",
       "      <td>3.068575e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <td>7.843213e+01</td>\n",
       "      <td>1.830099e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-JUS-CKGL</th>\n",
       "      <td>3.501117e+08</td>\n",
       "      <td>2.450589e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QGL</th>\n",
       "      <td>2.603752e-01</td>\n",
       "      <td>1.638849e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean_of_means  mean_of_stds\n",
       "P-PDG        1.650749e+07  1.201552e+07\n",
       "P-TPT        1.517584e+07  3.687992e+06\n",
       "T-TPT        1.061237e+02  1.629953e+01\n",
       "P-MON-CKP    4.729793e+06  3.068575e+06\n",
       "T-JUS-CKP    7.843213e+01  1.830099e+01\n",
       "P-JUS-CKGL   3.501117e+08  2.450589e+08\n",
       "QGL          2.603752e-01  1.638849e-01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cached metrics---standard deviation and average---to be used for data transformation\n",
    "mean_and_std_metric_cache_file_name = config.CACHE_NAME_TRAIN_MEAN_STD_DEV\n",
    "mean_and_std_metric_table = MetricAcquisition.get_mean_and_std_metric_from_cache(\n",
    "    mean_and_std_metric_cache_file_name\n",
    ")\n",
    "mean_metric_list = mean_and_std_metric_table['mean_of_means']\n",
    "std_metric_list = mean_and_std_metric_table['mean_of_stds']\n",
    "\n",
    "mean_and_std_metric_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar a transformação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:TransformationManager initialized with 1582 items.\n",
      "            Folder name is dataset_converted_v10101_split-20_source-all_class-all_well-all_train.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714c5cbe8e3340d48d534e835d7313f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DONE:   0%|          | 0/1582 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b70eeef964a4f4b9be65d1e7f030bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ERROR:   0%|          | 0/1582 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Exception while to split_sequences_into_windows. Path is: /home/ubuntu/lemi_3w/data/dataset_converted_v10101_split-20_source-all_class-all_well-all_train/5/SIMULATED_00046.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ubuntu/lemi_3w/data_preparation/transformation_manager.py\", line 353, in split_sequences_into_windows\n    y = keras.utils.to_categorical(\n  File \"/home/ubuntu/lemi_3w/.venv/lib/python3.10/site-packages/keras/src/utils/np_utils.py\", line 74, in to_categorical\n    categorical[np.arange(n), y] = 1\nIndexError: index 71 is out of bounds for axis 1 with size 9\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/lemi_3w/data_preparation/transformation_manager.py\", line 108, in apply_transformations_to_event\n    ) = TransformationManager.transform_event_with_timestep_windows(\n  File \"/home/ubuntu/lemi_3w/data_preparation/transformation_manager.py\", line 307, in transform_event_with_timestep_windows\n    return TransformationManager.split_sequences_into_windows(\n  File \"/home/ubuntu/lemi_3w/data_preparation/transformation_manager.py\", line 357, in split_sequences_into_windows\n    raise ValueError(\nValueError: Exception while to categorical. Present values: [ 5 71]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/home/ubuntu/lemi_3w/.venv/lib/python3.10/site-packages/parallelbar/parallelbar.py\", line 85, in func_wrapped\n    result = func(task)\n  File \"/home/ubuntu/lemi_3w/.venv/lib/python3.10/site-packages/parallelbar/tools.py\", line 11, in func_args_unpack\n    return func(*args)\n  File \"/home/ubuntu/lemi_3w/data_preparation/transformation_manager.py\", line 112, in apply_transformations_to_event\n    raise ValueError(\nValueError: Exception while to split_sequences_into_windows. Path is: /home/ubuntu/lemi_3w/data/dataset_converted_v10101_split-20_source-all_class-all_well-all_train/5/SIMULATED_00046.parquet\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_74518/440446457.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtransformation_param_num_timesteps_for_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m train_transformation_manager.apply_transformations_to_table(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moutput_parent_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDIR_PROJECT_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msample_interval_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformation_param_sample_interval_seconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lemi_3w/data_preparation/transformation_manager.py\u001b[0m in \u001b[0;36mapply_transformations_to_table\u001b[0;34m(self, output_parent_dir, sample_interval_seconds, num_timesteps_for_window, avg_variable_mean, avg_variable_std_dev)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# apply function to all of them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         progress_starmap(\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mTransformationManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_transformations_to_event\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             zip(\n",
      "\u001b[0;32m~/lemi_3w/.venv/lib/python3.10/site-packages/parallelbar/parallelbar.py\u001b[0m in \u001b[0;36mprogress_starmap\u001b[0;34m(func, tasks, initializer, initargs, n_cpu, chunk_size, context, total, bar_step, disable, process_timeout, error_behavior, set_error_value, executor, need_serialize, maxtasksperchild)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_args_unpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_func_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_serialize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     result = _do_parallel(func, 'map', tasks, initializer, initargs, n_cpu, chunk_size, context, total,\n\u001b[0m\u001b[1;32m    298\u001b[0m                           \u001b[0mbar_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_behavior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_error_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_serialize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                           \u001b[0mmaxtasksperchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lemi_3w/.venv/lib/python3.10/site-packages/parallelbar/parallelbar.py\u001b[0m in \u001b[0;36m_do_parallel\u001b[0;34m(func, pool_type, tasks, initializer, initargs, n_cpu, chunk_size, context, total, bar_step, disable, error_behavior, set_error_value, executor, need_serialize, maxtasksperchild)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mexc_pool\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpool_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'map'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         '''\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception while to split_sequences_into_windows. Path is: /home/ubuntu/lemi_3w/data/dataset_converted_v10101_split-20_source-all_class-all_well-all_train/5/SIMULATED_00046.parquet"
     ]
    }
   ],
   "source": [
    "train_tranformed_folder_name = split_train_dir.name\n",
    "\n",
    "train_transformation_manager = TransformationManager(\n",
    "    train_metadata.get_metadata_table(), \n",
    "    output_folder_base_name=train_tranformed_folder_name\n",
    ")\n",
    "\n",
    "transformation_param_sample_interval_seconds=60\n",
    "transformation_param_num_timesteps_for_window=20\n",
    "\n",
    "train_transformation_manager.apply_transformations_to_table(\n",
    "    output_parent_dir=config.DIR_PROJECT_DATA,\n",
    "    sample_interval_seconds=transformation_param_sample_interval_seconds,\n",
    "    num_timesteps_for_window=transformation_param_num_timesteps_for_window,\n",
    "    avg_variable_mean=mean_metric_list,\n",
    "    avg_variable_std_dev=std_metric_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/ubuntu/lemi_3w/data/dataset_converted_v10101_split-20_source-all_class-all_well-all_train/5/SIMULATED_00046.parquet\"\n",
    "\n",
    "utils.get_event(path)[\"class\"].isna().sum()\n",
    "\n",
    "#sum(np.isnan(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obter lista dos arquivos a serem usados no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transformed files paths\n",
    "train_tranformed_dataset_dir = (\n",
    "    config.DIR_PROJECT_DATA / \n",
    "    (TransformationManager.TRANSFORMATION_NAME_PREFIX + train_tranformed_folder_name))\n",
    "\n",
    "train_transformed_inspector = rdi.RawDataInspector(\n",
    "    train_tranformed_dataset_dir,\n",
    "    config.DIR_PROJECT_CACHE / \"inspector_transformed_all_data_train.parquet\",\n",
    "    True\n",
    ")\n",
    "train_transformed_metadata = train_transformed_inspector.get_metadata_table()\n",
    "train_transformed_data_file_path_list = train_transformed_metadata[\"path\"]\n",
    "\n",
    "\n",
    "def data_generator_loop(file_path_list):\n",
    "    \"\"\"Generator returning batches of data for each file path\"\"\"\n",
    "    while True:\n",
    "        for file_path in file_path_list:\n",
    "            X, y = TransformationManager.retrieve_pair_array(pathlib.Path(file_path))\n",
    "            yield X, y\n",
    "\n",
    "def data_generator_non_loop(file_path_list):\n",
    "    \"\"\"Generator returning batches of data for each file path\"\"\"\n",
    "    \n",
    "    for file_path in file_path_list:\n",
    "        X, y = TransformationManager.retrieve_pair_array(pathlib.Path(file_path))\n",
    "        yield X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de um par X, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_transformed_file_path = train_transformed_data_file_path_list[0]\n",
    "X_transformed_ex, y_transformed_ex = TransformationManager.retrieve_pair_array(pathlib.Path(example_transformed_file_path))\n",
    "\n",
    "X_transformed_ex[0], y_transformed_ex[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montagem da estrutura do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from constants import module_constants\n",
    "\n",
    "num_features = X_transformed_ex.shape[2]\n",
    "num_outputs = module_constants.num_class_types\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(transformation_param_num_timesteps_for_window, num_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(num_outputs, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "# times the generator (data_gen) should be called to complete one full pass through your training dataset\n",
    "steps_per_epoch = len(train_transformed_data_file_path_list)\n",
    "\n",
    "\n",
    "train_data_gen = data_generator_loop(train_transformed_data_file_path_list)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_data_gen, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=num_epochs, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver se está funcionando com um exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_file = train_transformed_data_file_path_list[-1]\n",
    "print(test_file)\n",
    "\n",
    "Xhat, yhat = TransformationManager.retrieve_pair_array(pathlib.Path(test_file))\n",
    "print(f\"True value: {yhat[0]}\")\n",
    "\n",
    "Xhat0 = Xhat[0].reshape(1, transformation_param_num_timesteps_for_window, num_features)\n",
    "print(f\"Predicted value: {model.predict(Xhat0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validação\n",
    "Aqui pegaremos nosso banco de testes, o transformaremos, para então o utilizar para validar a perfomance do nosso modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obter lista de arquivos usados para a validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tranformed_folder_name = split_test_dir.name\n",
    "\n",
    "test_transformation_manager = TransformationManager(\n",
    "    test_metadata.get_metadata_table(), \n",
    "    output_folder_base_name=test_tranformed_folder_name\n",
    ")\n",
    "\n",
    "test_transformation_manager.apply_transformations_to_table(\n",
    "    output_parent_dir=config.DIR_PROJECT_DATA,\n",
    "    sample_interval_seconds=transformation_param_sample_interval_seconds,\n",
    "    num_timesteps_for_window=transformation_param_num_timesteps_for_window,\n",
    "    avg_variable_mean=mean_metric_list,\n",
    "    avg_variable_std_dev=std_metric_list,\n",
    ")\n",
    "\n",
    "# Get transformed files paths\n",
    "test_tranformed_dataset_dir = (\n",
    "    config.DIR_PROJECT_DATA / \n",
    "    (TransformationManager.TRANSFORMATION_NAME_PREFIX + test_tranformed_folder_name)\n",
    ")\n",
    "\n",
    "test_transformed_inspector = rdi.RawDataInspector(\n",
    "    test_tranformed_dataset_dir,\n",
    "    config.DIR_PROJECT_CACHE / \"inspector_transformed_all_data_test.parquet\",\n",
    "    True\n",
    ")\n",
    "test_transformed_metadata = test_transformed_inspector.get_metadata_table()\n",
    "test_transformed_file_path_list = test_transformed_metadata[\"path\"]\n",
    "test_transformed_file_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen = data_generator_non_loop(test_transformed_file_path_list)\n",
    "num_steps = len(test_transformed_file_path_list)\n",
    "\n",
    "model.evaluate(\n",
    "    test_data_gen,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen = data_generator_non_loop(test_transformed_file_path_list)\n",
    "\n",
    "y_test_predictions = model.predict(\n",
    "    test_data_gen,\n",
    ")\n",
    "\n",
    "(\n",
    "    f\"Number of predictions: {len(y_test_predictions)}\", \n",
    "    f\"Shape of y array: {y_test_predictions.shape}\", \n",
    "    y_test_predictions[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen = data_generator_non_loop(test_transformed_file_path_list)\n",
    "y_test_labels = []\n",
    "\n",
    "for X, y in test_data_gen:\n",
    "    y_test_labels.append(y)\n",
    "\n",
    "y_test_labels = np.concatenate(y_test_labels, axis=0)\n",
    "\n",
    "(\n",
    "    f\"Number of predictions: {len(y_test_labels)}\", \n",
    "    f\"Shape of y array: {y_test_labels.shape}\", \n",
    "    y_test_labels[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.math import confusion_matrix\n",
    "\n",
    "y_test_labels_1d = np.argmax(y_test_labels, axis=1)\n",
    "y_test_predictions_1d = np.argmax(y_test_predictions, axis=1)\n",
    "\n",
    "confusion_matrix(\n",
    "    y_test_labels_1d,\n",
    "    y_test_predictions_1d,\n",
    "    num_classes=num_outputs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f76451b3a7719b8fd32adab79b436fd06da7a0268f14fa2684341b59d3c3b3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
